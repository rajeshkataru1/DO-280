Resource Manifests
Objectives
Deploy and update applications from resource manifests that are stored as YAML files.

An application in a Kubernetes cluster often consists of multiple resources that work together. Each resource has a definition and a configuration. Many of the resource configurations share common attributes that must match to operate correctly. Imperative commands configure each resource, one at time. However, using imperative commands has some issues:

Impaired reproducibility
Lacking version control
Lacking support for GitOps
Rather than imperative commands, declarative commands are instead the preferred way to manage resources, by using resource manifests. A resource manifest is a file, in JSON or YAML format, with resource definition and configuration information. Resource manifests simplify the management of Kubernetes resources, by encapsulating all the attributes of an application in a file or a set of related files. Kubernetes uses declarative commands to read the resource manifests and to apply changes to the cluster to meet the state that the resource manifest defines.

The resource manifests are in YAML or JSON format, and thus can be version-controlled. Version control of resource manifests enables tracing of configuration changes. As such, adverse changes can be rolled back to an earlier version to support recoverability.

Resource manifests ensure that applications can be precisely reproduced, typically with a single command to deploy many resources. The reproducibility from resource manifests supports the automation of the GitOps practices of continuous integration and continuous delivery (CI/CD).

Imperative Versus Declarative Workflows
The Kubernetes CLI uses both imperative and declarative commands. Imperative commands perform an action that is based on a command, and use command names that closely reflect the action. In contrast, declarative commands use a resource manifest file to declare the intended state of a resource.

A Kubernetes manifest is a YAML- or JSON-formatted file with declaration statements for Kubernetes resources such as deployments, pods, or services. Instead of using imperative commands to create Kubernetes resources, manifest files provide all the details for the resource in a single file. Working with manifest files enables the use of more reproducible processes. Instead of reproducing sequences of imperative commands, manifest files contain the entire definition of resources and can be applied in a single step. Using manifest files is also useful for tracking system configuration changes in a source code management system.

Given a new or updated resource manifest, Kubernetes provides commands that compare the intended state that is specified in the resource manifest to the current state of the resource. These commands then apply transformations to the current state to match the intended state.

Imperative Workflow
An imperative workflow is useful for developing and testing. The following example uses the kubectl create deployment imperative command, to create a deployment for a MYSQL database.

[user@host ~]$ kubectl create deployment db-pod --port 3306 \
  --image registry.ocp4.example.com:8443/rhel8/mysql-80
deployment.apps/db-pod created
In addition to using verbs that reflect the action of the command, imperative commands use options to provide the details. The example command uses the --port and the --image options to provide the required details to create the deployment.

The use of imperative commands affects applying changes to live resources. For example, the pod from the previous deployment would fail to start due to missing environment variables. The following kubectl set env deployment imperative command resolves the problem by adding the required environment variables to the deployment:

[user@host ~]$ kubectl set env deployment/db-pod \
  MYSQL_USER='user1' \
  MYSQL_PASSWORD='mypa55w0rd' \
  MYSQL_DATABASE='items'
deployment.apps/db-pod updated
Executing this kubectl set env deployment command changes the deployment resource named db-pod, and provides the extra needed variables to start the container. A developer can continue building out the application, by using imperative commands to add components, such as services, routes, volume mounts, and persistent volume claims. With the addition of each component, the developer can run tests to ensure that the component correctly executes the intended function.

Imperative commands are useful for developing and experimenting. With imperative commands, a developer can build up an application one component at a time. When a component is added, the Kubernetes cluster provides error messages that are specific to the component. The process is analogous to using a debugger to step through code execution one line at a time. Using imperative commands usually provides clearer error messages, because an error occurs after adding a specific component.

However, long command lines and a fragmented application deployment are not ideal for deploying an application in production. With imperative commands, changes are a sequence of commands that must be maintained to reflect the intended state of the resources. The sequence of commands must be tracked and kept up to date.

Using Declarative Commands
Instead of tracking a sequence of commands, a manifest file captures the intended state of the sequence. In contrast to using imperative commands, declarative commands use a manifest file, or a set of manifest files, to combine all the details for creating those components into YAML files that can be applied in a single command. Future changes to the manifest files require only reapplying the manifests. Instead of tracking a sequence of complex commands, version control systems can track changes to the manifest file.

Although manifest files can also use the JSON syntax, YAML is generally preferred and is more popular. To continue the debugging analogy, debugging an application that is deployed from manifests is similar to trying to debug a full, completed running application. It can take more effort to find the source of the error, especially when the error is not a result of manifest errors.

Creating Kubernetes Manifests
Creating manifest files from scratch can take time. You can use the following techniques to provide a starting point for your manifest files:

Use the YAML view of a resource from the web console.

Use imperative commands with the --dry-run=client option to generate manifests that correspond to the imperative command.

The kubectl explain command provides the details for any field in the manifest. For example, use the kubectl explain deployment.spec.template.spec command to view field descriptions that specify a pod object within a deployment manifest.

To create a starter deployment manifest, use the kubectl create deployment command to generate a manifest by using the --dry-run=client option:

[user@host ~]$ kubectl create deployment hello-openshift -o yaml \
  --image registry.ocp4.example.com:8443/redhattraining/hello-world-nginx:v1.0 \
  --save-config \ 1
  --dry-run=client \ 2
  > ~/my-app/example-deployment.yaml
1

The --save-config option adds configuration attributes that declarative commands use. For deployments resources, this option saves the resource configuration in an kubectl.kubernetes.io/last-applied-configuration annotation.

2

The --dry-run=client option prevents the command from creating resources in the cluster.

The following example shows a minimal deployment manifest file, not production-ready, for the hello-openshift deployment:

apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
      ...output omitted...
  creationTimestamp: null
  labels:
    app: hello-openshift
  name: hello-openshift
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hello-openshift
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: hello-openshift
    spec:
      containers:
      - image: quay.io/redhattraining/hello-world-nginx:v1.0
        name: hello-world-nginx
        resources: {}
status: {}
When using imperative commands to create manifests, the resulting manifests might contain fields that are not necessary for creating a resource. For example, the following example changes the manifest by removing the empty and null fields. Removing unnecessary fields can significantly reduce the length of the manifests, and in turn reduce the overhead to work with them.

Additionally, you might need to further customize the manifests. For example, in a deployment, you might customize the number of replicas, or declare the ports that the deployment provides. The following notes explain the additional changes:

apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: resource-manifests 1
  labels:
    app: hello-openshift
  name: hello-openshift
spec:
  replicas: 2 2
  selector:
    matchLabels:
      app: hello-openshift
  template:
    metadata:
      labels:
        app: hello-openshift
    spec:
      containers:
      - image: quay.io/redhattraining/hello-world-nginx:v1.0
        name: hello-world-nginx
        ports:
        - containerPort: 8080 3
          protocol: TCP
1

Add a namespace attribute to prevent deployment to the wrong project.

2

Requires two replicas instead of one.

3

Specifies the container port for the service to use.

You can create a manifest file for each resource that you manage. Alternatively, add each of the manifests to a single multi-part YAML file, and use a --- line to separate the manifests.

---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: resource-manifests
  annotations:
  ...output omitted...
---
apiVersion: v1
kind: Service
metadata:
  namespace: resource-manifests
  labels:
    app: hello-openshift
  name: hello-openshift
spec:
  ...output omitted...
Using a single file with multiple manifests versus using manifests that are defined in multiple manifest files is a matter of organizational preference. The single file approach has the advantage of keeping together related manifests. With the single file approach, it can be more convenient to change a resource that must be reflected across multiple manifests. In contrast, keeping manifests in multiple files can be more convenient for sharing resource definitions with others.

After creating manifests, you can test them in a non-production environment, or proceed to deploy the manifests. Validate the resource manifests before deploying applications in the production environment.

Declarative Workflows
Declarative commands use a resource manifest instead of adding the details to many options on the command line. To create a resource, use the kubectl create -f resource.yaml command. Instead of a file name, you can pass a directory to the command to process all the resource files in a directory. Add the --recursive=true or -R option to recursively process resource files that are provided in multiple subdirectories.

The following example creates the resources from the manifests in the my-app directory. In this example, the my-app directory contains the example-deployment.yaml and service/example-service.yaml files from previously.

[user@host ~]$ tree my-app
my-app
├── example_deployment.yaml
└── service
    └── example_service.yaml

[user@host ~]$ kubectl create -R -f ~/my-app
deployment.apps/hello-openshift created
service/hello-openshift created
The command also accepts a URL:

[user@host ~]$ kubectl create -f \
  https://example.com/example-apps/deployment.yaml
deployment.apps/hello-openshift created
Updating Resources
The kubectl apply command can also create resources with the same -f option that is illustrated with the kubectl create command. However, the kubectl apply command can also update a resource.

Updating resources is more complex than creating resources. The kubectl apply command implements several techniques to apply the updates without causing issues.

The kubectl apply command writes the contents of the configuration file to the kubectl.kubernetes.io/last-applied-configuration annotation. The kubectl create command can also generate this annotation by using the --save-config option. The kubectl apply command uses the last-applied-configuration annotation to identify fields that are removed from the configuration file and that must be cleared from the live configuration.

Although the kubectl create -f command can create resources from a manifest, the command is imperative and thus does not account for the current state of a live resource. Executing kubectl create -f against a manifest for a live resource gives an error. In contrast, the kubectl apply -f command is declarative, and considers the difference between the current resource state in the cluster and the intended resource state that is expressed in the manifest.

For example, to update the container's image from version v1.0 to latest, first update the YAML resource manifest to specify the new tag on the image. Then, use the kubectl apply command to instruct Kubernetes to create a version of the deployment resource by using the updated image version that is specified in the manifest.

YAML Validation
Before applying the changes to the resource, use the --dry-run=server and the --validate=true flags to inspect the file for errors.

The --dry-run=server option submits a server-side request without persisting the resource.

The --validate=true option uses a schema to validate the input and fails the request if it is invalid.

Any syntax errors in the YAML are included in the output. Most importantly, the --dry-run=server option prevents applying any changes to the Kubernetes runtime.

[user@host ~]$ kubectl apply -f ~/my-app/example-deployment.yaml \
  --dry-run=server --validate=true
deployment.apps/hello-openshift created (server dry-run) 1
1

The output line that ends in (server dry-run) provides the action that the resource file would perform if applied.

NOTE

The --dry-run=client option prints only the object that would be sent to the server. The cluster resource controllers can refuse a manifest even if the syntax is valid YAML. In contrast, the --dry-run=server option sends the request to the server to confirm that the manifest conforms to current server policies, without creating resources on the server.

Comparing Resources
Use the kubectl diff command to review differences between live objects and manifests. When updating resource manifests, you can track differences in the changed files. However, many manifest changes, when applied, do not change the state of the cluster resources. A text-based diff tool would show all such differences, and result in a noisy output.

In contrast, using the kubectl diff command might be more convenient to preview changes. The kubectl diff command emphasizes the significant changes for the Kubernetes cluster. Review the differences to validate that manifest changes have the intended effect.

[user@host ~]$ kubectl diff -f example-deployment.yaml
...output omitted...
diff -u -N /tmp/LIVE-2647853521/apps.v1.Deployment.resource...
--- /tmp/LIVE-2647853521/apps.v1.Deployment.resource-manife...
+++ /tmp/MERGED-2640652736/apps.v1.Deployment.resource-mani...
@@ -6,7 +6,7 @@
     kubectl.kubernetes.io/last-applied-configuration: |
       ...output omitted...
   creationTimestamp: "2023-04-27T16:07:47Z"
-  generation: 1 1
+  generation: 2
   labels:
     app: hello-openshift
   name: hello-openshift
@@ -32,7 +32,7 @@
         app: hello-openshift
     spec:
       containers:
-      - image: registry.ocp4.example.com:8443/.../hello-world-nginx:v1.0 2
+      - image: registry.ocp4.example.com:8443/.../hello-world-nginx:latest
         imagePullPolicy: IfNotPresent
         name: hello-openshift
         ports:
1

The line that starts with the - character shows that the current deployment is on generation 1. The following line, which starts with the + character, shows that the generation changes to 2 when the manifest file is applied.

2

The image line, which starts with the - character, shows that the current image uses the v1.0 version. The following line, which starts with the + character, shows a version change to latest when the manifest file is applied.

Kubernetes resource controllers automatically add annotations and attributes to the live resource that make the output of other text-based diff tools misleading, by reporting many differences that have no impact on the resource configuration. Extracting manifests from live resources and making comparisons with tools such as the diff command reports many differences of no value. Using the kubectl diff command confirms that a live resource matches a resource configuration that a manifest provides. GitOps tools depend on the kubectl diff command to determine whether anyone changed resources outside the GitOps workflow. Because the tools themselves cannot know all details about how any controllers might change a resource, the tools defer to the cluster to determine whether a change is meaningful.

Update Considerations
When using the oc diff command, recognize when applying a manifest change does not generate new pods. For example, if an updated manifest changes only values in secret or a configuration map, then applying the updated manifest does not generate new pods that use those values. Because pods read secret and configuration maps at startup, in this case applying the updated manifest leaves the pods in a vulnerable state, with stale values that are not synchronized with the updated secret or with the configuration map.

As a solution, use the oc rollout restart deployment deployment-name command to force a restart of the pods that are associated with the deployment. The forced restart generates pods that use the new values from the updated secret or configuration map.

In deployments with a single replica, you can also resolve the problem by deleting the pod. Kubernetes responds by automatically creating a pod to replace the deleted pod. However, for multiple replicas, using the oc rollout command to restart the pods is preferred, because the pods are stopped and replaced in a smart manner that minimizes downtime.

This course covers other resource management mechanisms that can automate or eliminate some of these challenges.

Applying Changes
The kubectl create command attempts to create the specified resources in the manifest file. Using the kubectl create command generates an error if the targeted resources are already live in the cluster. In contrast, the kubectl apply command compares three sources to determine how to process the request and to apply changes.

The manifest file

The live configuration of the resource in the cluster

The configuration that is stored in the last-applied-configuration annotation

If the specified resource in the manifest file does not exist, then the kubectl apply command creates the resource. If any fields in the last-applied-configuration annotation of the live resource are not present in the manifest, then the command removes those fields from the live configuration. After applying changes to the live resource, the kubectl apply command updates the last-applied-configuration annotation of the live resource to account for the change.

When creating a resource, the --save-config option of the kubectl create command produces the required annotations for future kubectl apply commands to operate.

Patching Kubernetes Resources
You can modify objects in OpenShift in a repeatable way with the oc patch command. The oc patch command updates or adds fields in an existing object from a provided JSON or YAML snippet or file. A software developer might distribute a patch file or snippet to fix problems before a full update is available.

To patch an object from a snippet, use the oc patch command with the -p option and the snippet. The following example updates the hello deployment to have a CPU resource request of 100m with a JSON snippet:

[user@host ~]$ oc patch deployment hello -p \
    '{"spec":{"template":{"spec":{"containers":[{"name": \
    "hello-rhel7","resources": {"requests": {"cpu": "100m"}}}]}}}}'
deployment/hello patched
To patch an object from a patch file, use the oc patch command with the --patch-file option and the location of the patch file. The following example updates the hello deployment to include the content of the ~/volume-mount.yaml patch file:

[user@host ~]$ oc patch deployment hello --patch-file ~/volume-mount.yaml
deployment.apps/hello patched
The contents of the patch file describe mounting a persistent volume claim as a volume:

spec:
  template:
    spec:
      containers:
        - name: hello
          volumeMounts:
            - name: www
              mountPath: /usr/share/nginx/html/
      volumes:
        - name: www
          persistentVolumeClaim:
            claimName: nginx-www
This patch results in the following manifest for the hello deployment:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello
  ...output omitted...
spec:
  ...output omitted...
  template:
    ...output omitted...
    spec:
      containers:
        ...output omitted...
        name: server
        ...output omitted...
        volumeMounts:
        - mountPath: /usr/share/nginx/html/
          name: www
        - mountPath: /etc/nginx/conf.d/
          name: tls-conf
      ...output omitted...
      volumes:
      - configMap:
          defaultMode: 420
          name: tls-conf
        name: tls-conf
      - persistentVolumeClaim:
          claimName: nginx-www
        name: www
...output omitted...
The patch applies to the hello deployment regardless of whether the www volume mount exists. The oc patch command modifies existing fields in the object that are specified in the patch:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello
  ...output omitted...
spec:
  ...output omitted...
  template:
    ...output omitted...
    spec:
      containers:
        ...output omitted...
        name: server
        ...output omitted...
        volumeMounts:
        - mountPath: /usr/share/nginx/www/ 1
          name: www
        - mountPath: /etc/nginx/conf.d/
          name: tls-conf
      ...output omitted...
      volumes:
      - configMap:
          defaultMode: 420
          name: tls-conf
        name: tls-conf
      - persistentVolumeClaim:
          claimName: deprecated-www 2
        name: www
...output omitted...
1 2

The www volume already exists. The patch replaces the existing data with the new data.
--------------
Guided Exercise: Resource Manifests
Deploy and update an application from resource manifests from YAML files that are stored in a Git server.

Outcomes

Deploy applications from resource manifests from YAML files that are stored in a GitLab repository.

Inspect new manifests for potential update issues.

Update application deployments from new YAML manifests.

Force the redeployment of pods when necessary.

As the student user on the workstation machine, use the lab command to prepare your system for this exercise.

[student@workstation ~]$ lab start declarative-manifests
Instructions

Log in to the OpenShift cluster and create the declarative-manifests project.

Log in to the cluster as the developer user.

[student@workstation ~]$ oc login -u developer -p developer \
  https://api.ocp4.example.com:6443
Login successful.

...output omitted...
Create the declarative-manifests project.

[student@workstation ~]$ oc new-project declarative-manifests
Now using project "declarative-manifests" on server ...
...output omitted...
Clone the declarative-manifest project from the Git repository.

Change your directory to the project labs directory.

[student@workstation ~]$ cd ~/DO280/labs
Clone the Git repository from https://git.ocp4.example.com/developer/declarative-manifests.git.

[student@workstation lab]$ git clone \
  https://git.ocp4.example.com/developer/declarative-manifests.git
Cloning into 'declarative-manifests'...
remote: Enumerating objects: 24, done.
remote: Counting objects: 100% (24/24), done.
remote: Compressing objects: 100% (21/21), done.
remote: Total 24 (delta 8), reused 0 (delta 0), pack-reused 0
Receiving objects: 100% (24/24), done.
Resolving deltas: 100% (8/8), done.
Go to the declarative-manifest directory.

[student@workstation lab]$ cd declarative-manifests
[student@workstation declarative-manifests]$
Inspect the contents of the Git repository.

List the contents of the declarative-manifests directory.

[student@workstation declarative-manifests]$ ls -lA
total 12
-rw-rw-r--. 1 student student 3443 Jun 21 16:39 database.yaml
-rw-rw-r--. 1 student student 2278 Jun 21 16:39 exoplanets.yaml
drwxrwxr-x. 8 student student  163 Jun 21 16:39 .git
-rw-rw-r--. 1 student student    0 Jun 21 16:39 .gitkeep
-rw-rw-r--. 1 student student   11 Jun 21 16:39 README.md
List the commits, branches, and tags on the Git repository.

[student@workstation declarative-manifests]$ git log --oneline
4045336 (HEAD -> main, tag: third, origin/v1.1.1, origin/main, origin/HEAD) Exoplanets v1.1.1  1
ad455b2 Database v1.1.1
821420c (tag: second, origin/v1.1.0) Exoplanets v1.1.0  2
d9abeb0 (tag: first, origin/v1.0) Exoplanets v1.0  3
a11396e Database v1.0
e868a90 README
18ddf3c Initial commit
1

The v1.1.1 branch points to the third version of the application.

2

The v1.1.0 branch points to the second version of the application.

3

The v1.0 branch points to the first version of the application.

Deploy the resource manifests of the first application version.

Switch to the v1.0 branch, which contains the YAML manifests for the first version of the application.

[student@workstation declarative-manifests]$ git checkout v1.0
branch 'v1.0' set up to track 'origin/v1.0'.
Switched to a new branch 'v1.0'
Validate the YAML resource manifest for the application.

[student@workstation declarative-manifests]$ oc apply -f . \
  --validate=true --dry-run=server
configmap/database created (server dry run)
secret/database created (server dry run)
deployment.apps/database created (server dry run)
service/database created (server dry run)
configmap/exoplanets created (server dry run)
secret/exoplanets created (server dry run)
deployment.apps/exoplanets created (server dry run)
service/exoplanets created (server dry run)
route.route.openshift.io/exoplanets created (server dry run)
Deploy the exoplanets application.

[student@workstation declarative-manifests]$ oc apply -f .
configmap/database created
secret/database created
deployment.apps/database created
service/database created
configmap/exoplanets created
secret/exoplanets created
deployment.apps/exoplanets created
service/exoplanets created
route.route.openshift.io/exoplanets created
List the deployments and pods. The exoplanets pod can go into a temporary crash loop backoff state if it attempts to access the database before it becomes available. Wait for the pods to be ready. Press Ctrl+C to exit the watch command.

[student@workstation declarative-manifests]$ watch oc get deployments,pods

Every 2.0s: oc get deployments,pods ...

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/database   1/1     1            1           32s
deployment.apps/exoplanets 1/1     1            1           32s

NAME                            READY   STATUS    RESTARTS   AGE
pod/database-6fddbbf94f-2pghj   1/1     Running   0          32s
pod/exoplanets-64c87f5796-bw8tm 1/1     Running   0          32s
List the route for the exoplanets application.

[student@workstation declarative-manifests]$ oc get routes -l app=exoplanets
NAME         HOST/PORT                                                ...
exoplanets   exoplanets-declarative-manifests.apps.ocp4.example.com   ...
Open the route URL in the web browser. The application version is v1.0.

http://exoplanets-declarative-manifests.apps.ocp4.example.com/

Deploy the second version of the exoplanets application.

Switch to the v1.1.0 branch of the Git repository.

[student@workstation declarative-manifests]$ git checkout v1.1.0
branch 'v1.1.0' set up to track 'origin/v1.1.0'.
Switched to a new branch 'v1.1.0'
Inspect the changes from the new manifests.

[student@workstation declarative-manifests]$ oc diff -f .
...output omitted...
         - secretRef:
             name: exoplanets
-        image: registry.ocp4.example.com:8443/redhattraining/exoplanets:v1.0
+        image: registry.ocp4.example.com:8443/redhattraining/exoplanets:v1.1.0
         imagePullPolicy: Always
         livenessProbe:
           failureThreshold: 3
The new version changes the image that is deployed to the cluster. Because the change is in the deployment, the new manifest produces new pods for the application.

Apply the changes from the new manifests.

[student@workstation declarative-manifests]$ oc apply -f .
configmap/database unchanged
secret/database configured
deployment.apps/database configured
service/database configured
configmap/exoplanets unchanged
secret/exoplanets configured
deployment.apps/exoplanets configured
service/exoplanets unchanged
route.route.openshift.io/exoplanets configured
List the deployments and pods. Wait for the application pod to be ready. Press Ctrl+C to exit the watch command.

[student@workstation declarative-manifests]$ watch oc get deployments,pods
Every 2.0s: oc get deployments,pods ...

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/database   1/1     1            1           6m32s
deployment.apps/exoplanets 1/1     1            1           6m33s

NAME                            READY   STATUS    RESTARTS   AGE
pod/database-6fddbbf94f-2pghj   1/1     Running   0          6m33s
pod/exoplanets-74c85f5796-tw8tf 1/1     Running   0          32s
List the route for the exoplanets application.

[student@workstation declarative-manifests]$ oc get routes -l app=exoplanets
NAME         HOST/PORT                                                ...
exoplanets   exoplanets-declarative-manifests.apps.ocp4.example.com   ...
Open the route URL in the web browser. The application version is v1.1.0.

http://exoplanets-declarative-manifests.apps.ocp4.example.com/
Deploy the third version of the exoplanets application.

Switch to the v1.1.1 branch of the Git repository.

[student@workstation declarative-manifests]$ git checkout v1.1.1
branch 'v1.1.1' set up to track 'origin/v1.1.1'.
Switched to a new branch 'v1.1.1'
View the differences between the currently deployed version of the application and the updated resource manifests.

[student@workstation declarative-manifests]$ oc diff -f .
...output omitted...
 kind: Secret 1
 metadata:
   annotations:
...output omitted...
-  DB_USER: '*** (before)' 2
+  DB_USER: '*** (after)'
 kind: Secret
 metadata:
   annotations:
1

The secret resource is changed.

2

The DB_USER field of the secret resource is changed.

Inspect the current application pods.

[student@workstation declarative-manifests]$ oc get pods
NAME                          READY   STATUS    RESTARTS   AGE
database-6fddbbf94f-brlj6     1/1     Running   0          44m
exoplanets-674cc57b5d-mv8kd   1/1     Running   0          18m
Deploy the new version of the application.

[student@workstation declarative-manifests]$ oc apply -f .
configmap/database unchanged
secret/database configured
deployment.apps/database configured
service/database configured
configmap/exoplanets unchanged
secret/exoplanets configured
deployment.apps/exoplanets unchanged
service/exoplanets unchanged
route.route.openshift.io/exoplanets configured
Inspect the current application pods again

[student@workstation declarative-manifests]$ oc get pods
NAME                          READY   STATUS             RESTARTS     AGE
database-6fddbbf94f-brlj6     1/1     Running            0            10m
exoplanets-674cc57b5d-mv8kd   0/1     CrashLoopBackOff   4 (14s ago)  2m
Although the secret is updated, the deployed application pods are not changed. These non-updated pods are a problem, because the pods load secrets and configuration maps at startup. Currently, the pods have stale values from the previous configuration, and therefore could crash.

Force the exoplanets application to restart, to flush out any stale configuration data.

Use the oc get deployments command to confirm the deployments.

[student@workstation declarative-manifests]$ oc get deployments
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
database     1/1     1            1           32m
exoplanets   0/1     1            0           32m
Use the oc rollout command to restart the database deployment.

[student@workstation declarative-manifests]$ oc rollout restart \
  deployment/database
deployment.apps/database restarted
Use the oc rollout command to restart the exoplanets deployment.

[student@workstation declarative-manifests]$ oc rollout restart \
  deployment/exoplanets
deployment.apps/exoplanets restarted
List the pods. The exoplanets pod can go into a temporary crash loop backoff state if it attempts to access the database before it becomes available. Wait for the application pod to be ready. Press Ctrl+C to exit the watch command.

[student@workstation declarative-manifests]$ watch oc get pods
Every 2.0s: oc get deployments,pods ...

NAME                          READY   STATUS    RESTARTS   AGEE
database-7c767c4bd7-m72nk     1/1     Running   0          32s
exoplanets-64c87f5796-bw8tm   1/1     Running   0          32s
Use the oc get deployment command with the -o yaml option to view the last-applied-configuration annotation.

[student@workstation declarative-manifests]$ oc get deployment \
  exoplanets -o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "3"
    description: Defines how to deploy the application server
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":...
    template.alpha.openshift.io/wait-for-ready: "true"
...output omitted...
Open the route URL in the web browser. The application version is v1.1.1.

http://exoplanets-declarative-manifests.apps.ocp4.example.com/
Clean up the resources.

Delete the application resources.

[student@workstation declarative-manifests]$ oc delete -f .
configmap "database" deleted
secret "database" deleted
deployment.apps "database" deleted
service "database" deleted
configmap "exoplanets" deleted
secret "exoplanets" deleted
deployment.apps "exoplanets" deleted
service "exoplanets" deleted
route.route.openshift.io "exoplanets" deleted
Change to the student HOME directory.

[student@workstation declarative-manifests]$ cd
[student@workstation ~]
-------------------
Kustomize Overlays
Objectives
Deploy and update applications from resource manifests that are augmented by Kustomize.

Kustomize
When using Kubernetes, multiple teams use multiple environments, such as development, staging, testing, and production, to deploy applications. These environments use applications with minor configuration changes.

Many organizations deploy a single application to multiple data centers for multiple teams and regions. Depending on the load, the organization needs a different number of replicas for every region. The organization might need various configurations that are specific to a data center or team.

All these use cases require a single set of manifests with multiple customizations at multiple levels. Kustomize can support such use cases.

Kustomize is a configuration management tool to make declarative changes to application configurations and components and preserve the original base YAML files. You group in a directory the Kubernetes resources that constitute your application, and then use Kustomize to copy and adapt these resource files to your environments and clusters. Both the kubectl and oc commands integrate the Kustomization tool.

Kustomize File Structure
Kustomize works on directories that contain a kustomization.yaml file at the root. Kustomize supports compositions and customization of different resources such as deployment, service, and secret. You can use patches to apply customization to different resources. Kustomize has a concept of base and overlays.

Base
A base directory contains a kustomization.yaml file. The kustomization.yaml file has a list resource field to include all resource files. As the name implies, all resources in the base directory are a common resource set. You can create a base application by composing all common resources from the base directory.

The following diagram shows the structure of a base directory:

base
├── configmap.yaml
├── deployment.yaml
├── secret.yaml
├── service.yaml
├── route.yaml
└── kustomization.yaml
The base directory has YAML files to create configuration map, deployment, service, secret, and route resources. The base directory also has a kustomization.yaml file, such as the following example:

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- configmap.yaml
- deployment.yaml
- secret.yaml
- service.yaml
- route.yaml
The kustomization.yaml file lists all resource files.

Overlays
Kustomize overlays declarative YAML artifacts, or patches, that override the general settings without modifying the original files. The overlay directory contains a kustomization.yaml file. The kustomization.yaml file can refer to one or more directories as bases. Multiple overlays can use a common base kustomization directory.

The following diagram shows the structure of all Kustomize directories:
The following example shows the directory structure of the frontend-app directory containing the base and overlay directories:

[user@host frontend-app]$ tree
base
├── configmap.yaml
├── deployment.yaml
├── secret.yaml
├── service.yaml
├── route.yaml
└── kustomization.yaml
overlay
└── development
    └── kustomization.yaml
└── testing
    └── kustomization.yaml
└── production
    ├── kustomization.yaml
    └── patch.yaml
The following example shows a kustomization.yaml file in the overlays/development directory:

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: dev-env
resources:
- ../../base
The frontend-app/overlay/development/kustomization.yaml file uses the base kustomization file at ../../base to create all the application resources in the dev-env namespace.

Kustomize provides fields to set values for all resources in the kustomization file:

Field	Description
namespace	Set a specific namespace for all resources.
namePrefix	Add a prefix to the name of all resources.
nameSuffix	Add a suffix to the name of all resources.
commonLabels	Add labels to all resources and selectors.
commonAnnotations	Add annotations to all resources and selectors.
You can customize for multiple environments by using overlays and patching. The patches mechanism has two elements: patch and target.

Previously, Kustomize used the PatchesJson6902 and PatchesStrategicMerge keys to add resource patches. These keys are deprecated in Kustomize version 5 and are replaced with a single key. However, the content of the patches key continues to use the same patch formats.

You can use JSON Patch and strategic merge patches. See the references section for further information about both patch formats.

The following is an example of a kustomization.yaml file in the overlays/testing directory:

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: test-env
patches: 1
- patch: |-
    - op: replace 2
      path: /metadata/name
      value: frontend-test
  target: 3
    kind: Deployment
    name: frontend
- patch: |- 4
    - op: replace
      path: /spec/replicas
      value: 15
  target:
    kind: Deployment
    name: frontend
resources: 5
- ../../base
commonLabels: 6
  env: test
1

The patches field contains a list of patches.

2

The patch field defines operation, path, and value keys. In this example, the name changes to frontend-test.

3

The target field specifies the kind and name of the resource to apply the patch. In this example, you are changing the frontend deployment name to frontend-test.

4

This patch updates the number of replicas of the frontend deployment.

5

The frontend-app/overlay/testing/kustomization.yaml file uses the base kustomization file at ../../base to create an application.

6

The commonLabels field adds the env: test label to all resources.

The patches mechanism also provides an option to include patches from a separate YAML file by using the path key.

The following example shows a kustomization.yaml file that uses a patch.yaml file:

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: prod-env
patches: 1
- path: patch.yaml 2
  target: 3
    kind: Deployment
    name: frontend
  options:
    allowNameChange: true 4
resources: 5
- ../../base
commonLabels: 6
  env: prod
1

The patches field lists the patches that are applied by using a production kustomization file.

2

The path field specifies the name of the patching YAML file.

3

The target field specifies the kind and name of the resource to apply the patch. In this example, you are targeting the frontend deployment.

4

The allowNameChange field enables kustomization to update the name by using a patch YAML file.

5

The frontend-app/overlay/production/kustomization.yaml file uses the base kustomization file at ../../base to create an application.

6

The commonLabels field adds an env: prod label to all resources.

The patch.yaml file has the following content:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-prod 1
spec:
  replicas: 5 2
1

The metadata.name field in the patch file updates the frontend deployment name to frontend-prod if the allowNameChange field is set to true in the kustomization YAML file.

2

The spec/replicas field in the patch file updates the number of replicas of the frontend-prod deployment.

View and Deploy Resources by Using Kustomize
Run the kubectl kustomize kustomization-directory command to render the manifests without applying them to the cluster.

[user@host frontend-app]$ kubectl kustomize overlay/production
...output omitted...
kind: Deployment
metadata:
  labels:
    app: frontend
    env: prod
  name: frontend-prod
...output omitted...
spec:
  replicas: 5
  selector:
    matchLabels:
      app: frontend
      env: prod
...output omitted...
The kubectl apply command applies configurations to the resources in the cluster. If resources are not available, then the kubectl apply command creates resources. The kubectl apply command applies a kustomization with the -k flag.

[user@host frontend-app]$ kubectl apply -k overlay/production
deployment.apps/frontend-prod created
...output omitted...
Delete Resources by Using Kustomize
Run the oc delete -k kustomization-directory command to delete the resources that were deployed by using Kustomize.

[user@host frontend-app]$ oc delete -k overlay/production
configmap "database" deleted
secret "database" deleted
service "database" deleted
deployment.apps "database" deleted
Kustomize Generators
Configuration maps hold non-confidential data by using a key-value pair. Secrets are similar to configuration maps, but secrets hold confidential information such as usernames and passwords. Kustomize has configMapGenerator and secretGenerator fields that generate configuration map and secret resources.

The configuration map and secret generators can include content from external files in the generated resources. By keeping the content of the generated resources outside the resource definitions, you can use files that other tools generated, or that are stored in different systems. Generators help to manage the content of configuration maps and secrets, by taking care of encoding and including content from other sources.

Configuration Map Generator
Kustomize provides a configMapGenerator field to create a configuration map. The configuration map that a configMapGenerator field creates behaves differently from configuration maps that are created without a Kustomize file. With generated configuration maps, Kustomize appends a hash to the name, and any change in the configuration map triggers a rolling update.

The following example adds a configuration map by using the configMapGenerator field in the staging kustomization file. The hello application deployment has two environment variables to refer to the hello-app-configmap configuration map.

The kustomization.yaml file has the following content:

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: hello-stage
resources:
- ../../base
configMapGenerator:
- name: hello-app-configmap
  literals:
    - msg="Welcome!"
    - enable="true"
The deployment.yaml file has the following content:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello
  labels:
    app: hello
    name: hello
spec:
...output omitted...
    spec:
      containers:
      - name: hello
        image: quay.io/hello-app:v1.0
        env:
        - name: MY_MESSAGE
          valueFrom:
            configMapKeyRef:
              name: hello-app-configmap
              key: msg
        - name: MSG_ENABLE
          valueFrom:
            configMapKeyRef:
              name: hello-app-configmap
              key: enable
You can view and deploy all resources and customizations that the kustomization YAML file defines, in the development directory.

[user@host hello-app]$ kubectl kustomize overlays/staging
apiVersion: v1
data:
  enable: "true"
  msg: Welcome!
kind: ConfigMap
metadata:
  name: hello-app-configmap-9tcmf95d77
  namespace: hello-stage
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hello
    name: hello
  name: hello
  namespace: hello-stage
spec:
...output omitted...
    spec:
      containers:
      - env:
        - name: MY_MESSAGE
          valueFrom:
            configMapKeyRef:
              key: msg
              name: hello-app-configmap-9tcmf95d77
        - name: MSG_ENABLE
          valueFrom:
            configMapKeyRef:
              key: enable
              name: hello-app-configmap-9tcmf95d77
...output omitted...

[user@host hello-app]$ kubectl apply -k overlays/staging
configmap/hello-app-configmap-9tcmf95d77 created
deployment.apps/hello created

[user@host hello-app]$ oc get all
NAME                         READY   STATUS    RESTARTS   AGE
pod/hello-75dc9cfc87-jh62k   1/1     Running   0          97s

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/hello   1/1     1            1           97s

NAME                               DESIRED   CURRENT   READY   AGE
replicaset.apps/hello-75dc9cfc87   1         1         1       97s
The kubectl apply -k command creates a hello-app-configmap-9tcmf95d77 configuration map and a hello deployment. Update the kustomization.yaml file with the configuration map values.

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: hello-stage
resources:
- ../../base
configMapGenerator:
- name: hello-app-configmap
  literals:
    - msg="Welcome Back!"
    - enable="true"
Then, apply the overlay with the kubectl apply command.

[user@host hello-app]$ kubectl apply -k overlays/staging
configmap/hello-app-configmap-696dm8h728 created
deployment.apps/hello configured

[user@host hello-app]$ oc get all
NAME                        READY   STATUS    RESTARTS   AGE
pod/hello-55bc55ff9-hrszh   1/1     Running   0          3s

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/hello   1/1     1            1           5m5s

NAME                               DESIRED   CURRENT   READY   AGE
replicaset.apps/hello-55bc55ff9    1         1         1       3s
replicaset.apps/hello-75dc9cfc87   0         0         0       5m5s
The kubectl apply -k command applies kustomization. Kustomize appends a new hash to the configuration map name, which creates a hello-app-configmap-696dm8h728 configuration map. The new configuration map triggers the generation of a new hello-55bc55ff9-hrszh pod.

You can generate a configuration map by using the files key from the .properties file or from the .env file by using the envs key with the file name as the value. You can also create a configuration map from a literal key-value pair by using the literals key.

The following example shows a kustomization.yaml file with the configMapGenerator field.

...output omitted...
configMapGenerator:
- name: configmap-1  1
  files:
    - application.properties
- name: configmap-2  2
  envs:
    - configmap-2.env
- name: configmap-3  3
  literals:
    - name="configmap-3"
    - description="literal key-value pair"
1

The configmap-1 key is using the application.properties file.

2

The configmap-2 key is using the configmap-2.env file.

3

The configmap-3 key is using a literal key-value pair.

The following example shows the application.properties file that is referenced in the configmap-1 key.

Day=Monday
Enable=True
The following example shows the configmap-2.env file that is referenced in the configmap-2 key.

Greet=Welcome
Enable=True
Run the kubectl kustomize command to view details of resources and customizations that the kustomization YAML file defines:

[user@host base]$ kubectl kustomize .
apiVersion: v1
data:
  application.properties: |
    Day=Monday
    Enable=True
kind: ConfigMap
metadata:
  name: configmap-1-5g2mh569b5 1
---
apiVersion: v1
data:
  Enable: "True"
  Greet: Welcome
kind: ConfigMap
metadata:
  name: configmap-2-92m84tg9kt 2
---
apiVersion: v1
data:
  description: literal key-value pair
  name: configmap-3
kind: ConfigMap
metadata:
  name: configmap-3-k7g7d5bffd 3
---
...output omitted...
1

The configMapGenerator field appends a hash to all ConfigMap resources. The configmap-1-5g2mh569b5 configuration map is generated from the application.properties file, and the data field has a single key with the application.properties value.

2

The configmap-2-92m84tg9kt configuration map is generated from the configmap-2.env file, and the data field has separate keys for each listed variable in the configmap-2.env file.

3

The configmap-3-k7g7d5bffd configuration map is generated from a literal key-value pair.

Secret Generator
A secret resource has sensitive data such as a username and a password. You can generate the secret by using the secretGenerator field. The secretGenerator field works similarly to the configMapGenerator field. However, the secretGenerator field also performs the base64 encoding that secret resources require.

The following example shows a kustomization.yaml file with the secretGenerator field:

...output omitted...
secretGenerator:
- name: secret-1  1
  files:
    - password.txt
- name: secret-2  2
  envs:
    - secret-mysql.env
- name: secret-3  3
  literals:
    - MYSQL_DB=mysql
    - MYSQL_PASS=root
1

The secret-1 key is using the password.txt file.

2

The secret-2 key is using the secret-mysql.env file.

3

The secret-3 key is using literal key-value pairs.

Generator Options
Kustomize provides a generatorOptions field to alter the default behavior of Kustomize generators. The configMapGenerator and secretGenerator fields append a hash suffix to the name of the generated resources.

Workload resources such as deployments do not detect any content changes to configuration maps and secrets. Any changes to a configuration map or secret do not apply automatically.

Because the generators append a hash, when you update the configuration map or secret, the resource name changes. This change triggers a rollout.

In some cases, the hash is not needed. Some operators observe the contents of the configuration maps and secrets that they use, and apply changes immediately. For example, the OpenShift OAuth operator applies changes to htpasswd secrets automatically. You can disable this feature with the generatorOptions field.

You can also add labels and annotations to the generated resources by using the generatorOptions field.

The following example shows the use of the generatorOptions field.

...output omitted...
configMapGenerator:
- name: my-configmap
  literals:
    - name="configmap-3"
    - description="literal key-value pair"
generatorOptions:
  disableNameSuffixHash: true
  labels:
    type: generated-disabled-suffix
  annotations:
    note: generated-disabled-suffix
You can use the kubectl kustomize command to render the changes to verify their effect.

[user@host base]$ kubectl kustomize .
apiVersion: v1
data:
  description: literal key-value pair
  name: configmap-3
kind: ConfigMap
metadata:
  annotations:
    note: generated-disabled-suffix
  labels:
    type: generated-disabled-suffix
  name: my-configmap
The my-configmap configuration map is without a hash suffix, and has a label and annotations that are defined in the kustomization file.

-------------------
